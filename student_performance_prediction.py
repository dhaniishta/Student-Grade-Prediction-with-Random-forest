# -*- coding: utf-8 -*-
"""student_performance_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sClqZf1O7XoHtBmCVLWoVq_4sT02Epal
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

math_df= pd.read_csv('/content/student-mat.csv',sep=';')
pd_df= pd.read_csv('/content/student-por.csv',sep=';')

data=pd.concat([math_df,pd_df]).drop_duplicates()

data.head()

categorical_cols= [col for col in data.columns if data[col].dtype=='O']
categorical_cols
encoder=OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded=encoder.fit_transform(data[categorical_cols])
categorical_df=pd.DataFrame(categorical_encoded,columns=encoder.get_feature_names_out(categorical_cols))
categorical_df
#data.head()
#categorical_cols

#data = data.drop(columns=[col for col in categorical_cols if col in data.columns]).reset_index(drop=True)
data = data.drop(columns=categorical_cols).reset_index(drop=True)
categorical_df=categorical_df.reset_index(drop=True)
data=pd.concat([data,categorical_df],axis=1)
data.head()

target_corr = data.corr()['G3'].abs().sort_values(ascending=False)
target_corr

drop_col=target_corr[target_corr<0.05].index.tolist()
data=data.drop(columns=drop_col)
data.head()

x=data.drop(columns=['G3'])
y=data['G3']

x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=42)

scaler=StandardScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)

rf=RandomForestRegressor(n_estimators=100,random_state=42)
rf.fit(x_train,y_train)

y_pred=rf.predict(x_test)

mae=mean_absolute_error(y_test,y_pred)
mse=mean_squared_error(y_test,y_pred)
r2=r2_score(y_test,y_pred)
print(mae,mse,r2)

feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 5))
sns.barplot(x=feature_importances[:10], y=feature_importances.index[:10])
plt.title("Top 10 Important Features")
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load datasets
math_df = pd.read_csv('/content/student-mat.csv',sep=';')
port_df = pd.read_csv('/content/student-por.csv',sep=';')

# Merge datasets (Optional: Keep unique students only)
data = pd.concat([math_df, port_df]).drop_duplicates()

# Drop unnecessary columns
drop_cols = ['school', 'guardian']  # Adjust based on correlation analysis
data = data.drop(columns=drop_cols)

# Handle categorical variables
categorical_cols = [col for col in data.columns if data[col].dtype == 'O']
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_encoded = encoder.fit_transform(data[categorical_cols])
categorical_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_cols))

# Ensure unique column names
categorical_df.columns = [f"{col}_{i}" for i, col in enumerate(categorical_df.columns)]

# Replace categorical columns with encoded values
data = data.drop(columns=categorical_cols).reset_index(drop=True)
categorical_df = categorical_df.reset_index(drop=True)
data = pd.concat([data, categorical_df], axis=1)

# Define target variable (G3 - Final Grade)
X = data.drop(columns=['G3'])
y = data['G3']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predictions
y_pred = rf.predict(X_test)

# Evaluation
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.2f}")

# Feature Importance
feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 5))
sns.barplot(x=feature_importances[:10], y=feature_importances.index[:10])
plt.title("Top 10 Important Features")
plt.show()

